https://leetcode.com/problems/network-delay-time/submissions/1803255650/

class Solution {
public:
    int networkDelayTime(vector<vector<int>>& times, int n, int k) {
        vector < vector<pair<int, int>>> adj(n + 1);
        for (auto& edge : times) {
            int u = edge[0], v = edge[1], w = edge[2];
            adj[u].push_back({v, w});
        }
        vector<int> dist(n + 1, INT_MAX);
        dist[k] = 0;
        priority_queue<pair<int, int>, vector<pair<int, int>>, greater<>> pq;
        pq.push({0, k});
        while (!pq.empty()) {
            auto [time, u] = pq.top();
            pq.pop();
            if (time > dist[u])
                continue;
            for (auto& [v, w] : adj[u]) {
                if (dist[v] > dist[u] + w) {
                    dist[v] = dist[u] + w;
                    pq.push({dist[v], v});
                }
            }
        }
        int maxtime = 0;
        for (int i = 1; i <= n; i++) {
            if (dist[i] == INT_MAX)
                return -1;
            maxtime = max(maxtime, dist[i]);
        }
        return maxtime;
    }
};
